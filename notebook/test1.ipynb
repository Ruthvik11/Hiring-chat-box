{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3ceb1b2-c956-4702-a06d-5da38579a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5cb885d-ca4d-4f53-b60c-4290b1b78ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-large\"\n",
    "token = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83981482-3972-4fc2-bb2f-ebc4a34f0e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_flow = [\n",
    "    \"Hi! I am Your Hiring Assistant\",\n",
    "    \"What is your full name? \",\n",
    "    \"What is your mobile number? \",\n",
    "    \"What is your email address? \",\n",
    "    \"How many years of experience do you have? \",\n",
    "    \"What position are you applying for? \",\n",
    "    \"Tech Stack: what are your technical skills (languages, frameworks, databases, tools)? \"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab222015-16fc-46a3-8ddf-63cc3d05b7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! I am Your Hiring Assistant\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What is your full name?  bf hwkjf\n",
      "What is your mobile number?  9300494\n",
      "What is your email address?  skfjnjsfn\n",
      "How many years of experience do you have?  2\n",
      "What position are you applying for?  web developer\n",
      "Tech Stack: what are your technical skills (languages, frameworks, databases, tools)?  java,flask,mysql\n"
     ]
    }
   ],
   "source": [
    "# Collect user data\n",
    "user_data = {}\n",
    "print(chat_flow[0])\n",
    "user_data[\"name\"] = input(chat_flow[1])\n",
    "user_data[\"mobile\"] = input(chat_flow[2])\n",
    "user_data[\"email\"] = input(chat_flow[3])\n",
    "user_data[\"exp\"] = input(chat_flow[4])\n",
    "user_data[\"jobrole\"] = input(chat_flow[5])\n",
    "raw_stack = [t.strip().lower() for t in input(chat_flow[6]).split(\",\") if t.strip()]\n",
    "# Filter valid tech (exclude tools like VSCode)\n",
    "valid_tech = {\n",
    "    \"languages\": [\"python\", \"javascript\", \"java\", \"c++\", \"ruby\"],\n",
    "    \"frameworks\": [\"django\", \"flask\", \"react\", \"angular\", \"spring\"],\n",
    "    \"databases\": [\"sql\", \"mysql\", \"postgresql\", \"mongodb\"]\n",
    "}\n",
    "tech_stack = [t for t in raw_stack if t in sum(valid_tech.values(), [])]\n",
    "user_data[\"teck_stack\"] = \", \".join(tech_stack) if tech_stack else \"python\"\n",
    "tech_list = tech_stack if tech_stack else [\"python\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1142a41c-20c0-4891-9103-824ee63e8a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Hiring Chatbox\\env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "D:\\Hiring Chatbox\\env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Generate questions\n",
    "questions = []\n",
    "aspects = [\n",
    "    \"core technical concepts (e.g., syntax, architecture, or mechanisms)\",\n",
    "    \"practical coding or implementation (e.g., writing functions, components, or queries)\",\n",
    "    \"debugging or performance optimization (e.g., fixing issues or improving efficiency)\"\n",
    "]\n",
    "for i, aspect in enumerate(aspects):\n",
    "    tech = tech_list[i % len(tech_list)]  # Cycle through techs\n",
    "    prompt = f\"\"\"\n",
    "    You are a technical hiring assistant for a recruitment agency. Generate **one technical interview question** to evaluate a candidate's proficiency in {tech}. The question must:\n",
    "    - Focus strictly on {aspect}.\n",
    "    - Be highly specific to {tech}â€™s technical features, avoiding general or experience-based queries (e.g., do not ask about duration of use).\n",
    "    - Suit the experience level (beginner for <2 years, intermediate for 2-5 years, advanced for >5 years).\n",
    "    - Align with the position: {user_data['jobrole']}.\n",
    "    - Be concise and testable in an interview.\n",
    "\n",
    "    Candidate Details:\n",
    "    - Experience: {user_data['exp']}\n",
    "    - Position: {user_data['jobrole']}\n",
    "\n",
    "    Output Format:\n",
    "    {i+1}. [Question]\n",
    "    \"\"\"\n",
    "    inputs = token(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=200,      # Increased for detail\n",
    "            num_beams=10,       # Higher quality\n",
    "            temperature=0.9,    # Balanced diversity\n",
    "            top_p=0.9,\n",
    "            no_repeat_ngram_size=2,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    question = token.decode(outputs[0], skip_special_tokens=True)\n",
    "    questions.append(question.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f8940b1-da0d-4f7f-86b7-73e8710183eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse and ensure 3 questions\n",
    "parsed_questions = []\n",
    "for q in questions:\n",
    "    q = q.strip()\n",
    "    if q and (q.startswith((\"1.\", \"2.\", \"3.\")) or len(q) > 20):  # Accept longer non-numbered questions\n",
    "        parsed_questions.append(q if q.startswith((\"1.\", \"2.\", \"3.\")) else f\"{len(parsed_questions)+1}. {q}\")\n",
    "\n",
    "if len(parsed_questions) != 3:\n",
    "    # Fallback\n",
    "    tech_list = tech_list if tech_list else [\"python\"]\n",
    "    parsed_questions = [\n",
    "        f\"1. Explain a core concept of {tech_list[0]}.\",\n",
    "        f\"2. Write a simple function using {tech_list[min(1, len(tech_list)-1)]}.\",\n",
    "        f\"3. How would you debug an issue in {tech_list[min(2, len(tech_list)-1)]}?\"\n",
    "    ][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd1cfc56-a54f-4ea6-b21a-1cdc029cd2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Technical Questions:\n",
      "1. How long have you been using java?\n",
      "2. How long have you been using flask?\n",
      "3. How long have you been using mysql?\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTechnical Questions:\")\n",
    "print(\"\\n\".join(parsed_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f5eb84-e2b9-4a23-8a60-432110541dae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
